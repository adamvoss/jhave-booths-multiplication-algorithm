\documentclass{article}

\usepackage{amsbsy,amscd,amsfonts,amsgen,amsmath,amsopn,amssymb,amstext,amsthm,amsxtra}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage[utf8]{inputenc}

%-------------------------------------------------------------------
% You can insert user-defined macros 
\DeclareUnicodeCharacter{03B2}{\beta}
%-------------------------------------------------------------------

\title{Booth's Algorithm for the Multiplication of Signed Binary Numbers}
\date{\today}
\author{Christopher Jenkins\\ Trinity University, cjenkin1@trinity.edu
    \and Adam Voss\\ Luther College, vossad01@luther.edu}

\begin{document}

\maketitle
\tableofcontents

\pagebreak

\section{Objectives and Preparation}
After completing this section you should understand the Booth's Multiplication Algorithm for the multiplication of signed binary numbers.
Booth's Multiplication Algorithm provides a simple and relatively efficient algorithm for using machine-register operations for the multiplication of two's complement binary values.

This sections assumes familiarity with with binary addition at the machine level and with the two's complement representations of negative numbers.
Additionally, it is recommended that a reading of this text is accompanied by Booth's Multiplication Algorithm Visualization, available on the Java-Hosted Algorithm Visualization Environment (JHAVÃ‰).
For information regarding the use of this tool, consult ``A Guide for Visualizing Booth's Multiplication Algorithm''.
%Possibly make recommendations here

\section{Introduction}
The process of binary multiplication is significantly more computationally complex than that of addition.
Multiplication is often defined recursively as repeated additions.
%Consider providing full definition|Perhaps, but it seems implied in the following example
Using this approach to calculate $11 \times 23$, you would start with 0 and add 11 to it 23 times.
This method is inefficient, and execution time varies depending on the values multiplied.
For example, the equivalent problem of calculating $23 \times 11$ requires 12 fewer operations.
Since as computer architects we prefer predictability to unpredictability, we would like to make several modifications to this algorithm so that it resembles something that could be done well in hardware.
%There is not background to this statement of Architects and efficiency.|I didn't think one was required - it seems intuitive

%This section seems math-y, maybe should be re-arranged in the article
\section{Shift-and-Add Multiplication}
The ``shift-and-add'' method for binary multiplication is the binary-equivalent of multiplication by partial products.
%Is this paragraph necessary? I think so. It's probably not a bad idea to be explicit, since "partial products" or "shift and add" may not be very familiar to the student
To review, multiplication by partial products involves multiplying one number, hereafter called the multiplicand, or $M$, by successively more significant digits of the other number, hereafter referred to as the multiplier, or $Q$.
The results of these individual multiplications are referred to as ``partial products'', and the sum of these partial products is the final product.

%This paragraph is very much draft quality, but I thought it important to show where the name came from
% as this provides more background as well at introducing hardware for parity with how we present Booth.

%If this is not what you wanted for shift and add, then maybe present it as partial products
%Then use the next section to provide how partial products translates to shifts and add in HW
\begin{comment}
When translated to binary, multiplication by partial products is reduced to a series of shift and add operation which is where the name comes from.
The product is kept in an initially zeroed register with each partial sum added to it.
As the multiplicand bits are traversed from least-significant to most-significant, every succession is accompanied by a left-shift of the Multiplicand.
%Maybe tie in here that the sift is equivalent to *2, which is the binary equivalent of decimal *10 (add a zero to the end)
If the current bit of the multiplier is 1, the current, shifted Multiplicand is added into the Sum register.
\end{comment}
%Oh dear, the merge is going to be harder than I thought, since I already developed some of this stuff later.

Another way to think of this is as follows: let $Q$ be an n-digit binary number and let the $i^{th}$ bit of $Q$ be denoted $q_i$ (this text goes by the convention that the $0^{th}$ bit of a binary value is the rightmost or least-significant bit).
Then the significance (or ``place-value'' as you learned it in elementary school) of $q_i$ is $2^i$.
We can then express the problem $M \times Q$ as $M \times (q_{n-1} \times 2^{n-1} + q_{n-2} \times 2^{n-2} + \ldots + q_{0} \times 2^{0})$.
Each one of the products $(q_i \times 2^{i}) \times M$ is a partial product.
For binary values, there are only two values a partial product may be: if $q_i = 0$, then the result is simply $0$; otherwise the result is $M \times 2^i$.

\pagebreak

Consider the following calculation of the multiplication of 001011 and 010111, which are the numbers 11 and 23 in twos complement notation, respectively.
Since many computer architectures use fixed-sized registers to hold data, we will use the same number of bits to represent both $M$ and $Q$.

%diagram goes here
\begin{figure}[h]
\centering
\includegraphics[scale=0.7]{saam3.pdf}
\caption{Shift-and-Add Multiplication}
\end{figure}

We see already an improvement in the performance of this algorithm, compared to the repeated additions method.
Each partial product shown corresponds to an addition operation (+M or +0).
In every line except the first, there is also an implicit shift operation, corresponding to the significance of the bit being multiplied with $M$.
Again, recall that the significance of bit $q_i$ is $2^i$, so if $q_i = 1$, then the partial product looks like $M$ followed by $i$ trailing 0s.
Much like multiplying by powers of $10$ in decimal, we are simply ``shifting'' the bits in M to more significant place values.

Compare the method described above, which involves the addition of 5 partial products and 4 shifts for this problem, to the method proposed in the introduction, which requires 23 additions.
Even though this is only one example, it should give you a good idea of how much better the performance of this algorithm is than the first.
The speedup is even greater if one considers that, on some machines, shift operations can potentially take less time than addition operations.

\section{Problems with the Shift-and-Add Method}
There are several issues that prevent us from directly implementing the shift-and-add algorithm into hardware.
To begin with, most ALUs are only capable of adding two numbers together at a time.
This means a \emph{running product} must be calculated for every partial product, instead of calculating the final product at the very end of the process.
The running product is initialized to 0, and for every partial product calculated the new running product is the sum of the old running product plus the partial product.
We will call the space used to store the running product register A.
Since we are beginning our discussion of computer design problems, we will similarly refer to the space used to store the multiplicand and multiplier as register M and register Q, respectively.

Notice also that as the algorithm progresses, the position at which the numbers are added incrementally slides to the left.
It is a much simpler task to design a machine that always adds into the same location and then shifts the running product to the right by 1.
We will refer to this operation as a \emph{sign-preserving right shift}, or \emph{sprs} for short.
This is also known as an \emph{arithmetic right shift}.

Finally, we need to consider register size.
In the previous example, the numbers we multiplied were represented as 6-digit binary values; their result, however, required 10 bits to represent.
In general, if two n-bit numbers are multiplied, then the result can require as many as 2n bits to represent.
Thus, the convention is to store the result of multiplication between two registers.

\begin{figure}[h]
\centering
\includegraphics[scale=0.7]{isaam2.pdf}
\caption{Improved Shift-and-Add Multiplication}
\end{figure}

\pagebreak

\section{Improved Shift-and-Add Multiplication}
Figure 2 shows an improved version of the shift-and-add multiplication algorithm.
You should study it so that you understand how the problems which were discussed in the previous section are addressed in this new shift and add method.

There is still another computer design issue which could make this method easier to implement in hardware: in the last example, the bit in register Q we are using to calculate a partial product is always positioned one to the left of the previous bit we used, or else it is the right-most (least significant, \emph{lsb}) bit of register Q when the multiplication starts.
We could instead choose to always examine the \emph{lsb} of register Q, and for every right shift operation on register A, we would also do a right shift on register Q.
Thus, when the algorithm finishes, all of the previous values in Q will have been discarded.
This will allow us to be even more efficient: instead of using another register to store the lower half of the product, we can use register Q, because, for every bit discarded from Q, we can place into Q's most significant bit (\emph{msb}) the \emph{lsb} of A, saving ourselves from using a whole other register!

\begin{figure}[h]
\centering
\includegraphics{init.pdf}
\caption{Registers M, A, and Q, initialized for the multiplication of 11 and 23}
\end{figure}

\pagebreak

\section{The Problem with Negative Numbers}
Consider the multiplication of the following 4-bit 2's complement numbers: 0011 (3) and 1011 (-5).
We expect a result of -15 (11110001); however, currently the method we have been developing is unable to distinguish between signed and unsigned values, and would multiply 3 by 11 (the unsigned decimal value of 1011).
One way around this behavior would be to calculate the absolute values of our multiplier and multiplicand.
Then, after the positive product was calculated, if either the multiplicand or the multiplier are negative (but not both), calculate the two's complement of the product.
This is a suboptimal solution from a design point of view, as it introduces the need for extra hardware and/or processing of binary numbers whenever we wish to multiply.

\section{Booth's Encoding}
In 1951, Andrew Booth introduced an alternative method for multiplying two's complement numbers.
At its heart is the observation that a number like 0111, which is understood to mean $2^2 + 2^1 + 2^0 = 4 + 2 + 1 = 7$, can also be thought of as $2^3 - 2^0 = 8 - 1 = 7$.
This can be written (encoded) as $100\bar{1}$, where 1 and 0 have their understood meanings, and $\bar{1}$ represents $-1$.
Generally, it can be shown that any sequence $2^n + 2^{n-1} + ...
+ 2^{n-k}$ (where $k+1$ is the number of terms in the sequence) is equal to $2^{n+1} - 2^{n-k}$

To encode an $n$-digit binary number $m$, examine the $i^{th}$ digit of $m$ (we will call it $m_i$) and do one of the following: %TODO citation needed for this

\begin{enumerate}
\item If $m_i$ and $m_{i-1}$ (the digit to the left of $m_i$) are 1 and 1, or they are 0 and 0, write 0 at the $i^{th}$ digit in the new encoded value.
\item If $m_i$ and $m_{i-1}$ are 0 and 1 respectively, write 1 at the the $i^{th}$ digit in the new encoded value.
\item If $m_i$ and $m_{i-1}$ are 1 and 0 respectively, write $\bar{1}$ at the $i^{th}$ digit in the new encoded value.
\item When $i = 0$, assume $m_{i-1}$ is 0
\end{enumerate}

Take for example the binary number 011100011, which is $2^7 + 2^6 + 2^5 + 2^1 + 2^0 = 227$.
Applying the method described above, our encoded value for this number is $100\bar{1}0010\bar{1}$ which is $2^8 - 2^5 + 2^2 - 2^0 = 227$
In this way, the method takes a ``run'' of $1$s in a binary value (representing the series $2^{n} + 2^{n-1} + ...
+ 2^{n-k}$), takes the first 0 bit to the left of the run and turns it into $1$ and turns the entire run into $0$s except for the last $1$ in the run, which is turned into $\bar{1}$.

How does this solve the problem of negative numbers?
Remember that in a two's complement binary value the \emph{msb} represents a subtraction of the highest power of two.
For example, $11100100$ represents the value $-(2^{7}) + 2^{6} + 2^{5} + 2^{2} = -128 + 64 + 32 + 4 = -28$
Applying the method described above, we obtain an encoded value of $00\bar{1}01\bar{1}00$, which represents $-(2^{5}) + 2^{3} - 2^{2} = -32 + 8 - 4 = -28$.
Generally, if the \emph{msb} of an n-digit two's complement binary number is 1, then the leftmost ``run'' of $1$s represents the value $-(2^{n-1}) + 2^{n-2} + ...
+ 2^{(n-1)-k}$.
Using the mathematical insight of Booth's Encoding technique, we convert this to $-(2^{n-1}) + (2^{n-1}) - 2^{(n-1)-k} = -2^{(n-1)-k}$.
(Our sequence in this case was $2^{n-2} + 2^{n-3} + ...
+ 2^{(n-1)-k}$)
Because Booth's Encoding is based on this mathematical equivalence, we are guaranteed to preserve the value of the candidate number, whether positive or negative.
 
%TODO this belongs in hardware discussion
%This property has the potential to further increase efficiency, as any arbitrary ``run'' of $1$s (series of powers of 2) will be reduced to just 2 arithmetic operations, since when we encounter $0$s we need only shift.

%How does this solve negative numbers?
%Hadn't gotten there yet

\section{Booth's Algorithm in Hardware}
    At the hardware level we are only allowed the use of 0 and 1 to represent values, so we instead examine pairs of bits.
Scanning a binary number from right to left, the pair `10' signifies the beginning of a sequence of 1s and thus corresponds to a subtraction (or $\bar{1}$).
The pair `01' signifies the end of a sequence of 1s and corresponds to an addition.
The pairs '00' and '11' signify that no arithmetic operations need occur (the equivalent of adding 0).
Since we would like to stay as close as possible to the algorithm outlined earlier, all cases will still require a shift.

    In order to examine pairs of bits, we add an extra 1 bit register, called $\beta$ (or Beta) to hold the bit shifted out of register Q.
$\beta$ is initialized to 0, so that at the start of the algorithm, if the least significant bit of Q is 1, it will read a pair `10' and execute a subtraction.
Keep in mind now that whenever a shift occurs, the least significant bit of A is moved into the most significant bit of Q, and the least significant bit of Q is moved into $\beta$.
The old value of $\beta$ is discarded.

\begin{figure}[h]
\centering
\includegraphics{init2.pdf}
\caption{Registers M, A, Q, and Beta, initialized for the multiplication of 11 and 23}
\end{figure}

\pagebreak

\section{Visualizing Booth's Multiplication}
Everything we have considered so far for an appropriate machine level algorithm to multiply two's complement values has been implemented as a JHAVÃ‰ algorithm visualization.
Instead of using the form of the previous examples, it more closely resembles the way the algorithm might be executed in hardware, placing explicitly the multiplicand and multiplier in registers M and Q respectively, and storing the final result in registers A and Q.
Familiarize yourself with the format of the visualization by consulting ``A Guide for Visualizing Boothâ€™s Multiplication Algorithm'', then step through the visualization as many times as you need to understand how the algorithm works, answering the questions as they appear.
Once you have finished, proceed to the exercises, detailed below.

\section{Exercises}
For each of the following exercises, there is a corresponding visualization with the same name available on JHAVÃ‰.
We suggest solving the exercises by entering your answers into the visualization's ``input generator'', as the visualization is designed to judge the correctness of your solution and provided feedback if you have made a mistake.

\begin{description}
    \item [Exercise 1] Initialize Booth's Multiplication Algorithm for the multiplication of numbers X and Y by giving the values of the registers M, A, and Q, the bit $\beta$, and Count
    \item [Exercise 2] Complete Booth's Multiplication Algorithm for the multiplication of numbers X and Y by initializing the values of the registers M, A, Q, the bit $\beta$, and Count, and then giving the values of each at the end of each iteration of the algorithm's loop
    \item [Exercise 3] Give numbers X and Y, either in binary or in decimal, so that they will result in the least efficient performance of Booth's Algorithm for $n$-digit binary numbers
\end{description}
%For your reference, the previous states of the registers are kept on screen as the visualization progresses.
%They are grayed to indicate they are no longer the active values of the registers.
%An example run-through is provided below, as well as a link to a use-case video.%TODO this
%Step through the visualization as many times as you need, answering the questions as they appear, then proceed on to the exercises.

%\section{Example Run of the Visualization}
%After starting the JHAVÃ‰ client and selecting ``Booth's Multiplication Algorithm'', an input generator window will appear with four text fields, two for decimal input and two four binary input, and a menu to select register size.
%You may choose your own values for the visualization, or use the default values provided (if you are having trouble getting the input generator to take your values, make sure you selected an appropriate register size for your values).
%When you are ready to begin the visualization, press ``OK''.

%\begin{figure}[h]
%\centering
%\includegraphics{vis1.pdf}
%\end{figure}

%The next few snapshots of the visualization 

%\section{How it works}
%-------------------------------------------------------------------
% to create references, un-comment \bibliographystyle{plain} and
% un-comment \bibliography{myBIBfile} and re-name its arguemnt(s)
% to point at the .bib file(s) containing the BibTeX references:

%\bibliographystyle{plain}
%\bibliography{myBIBfile}

\end{document}
